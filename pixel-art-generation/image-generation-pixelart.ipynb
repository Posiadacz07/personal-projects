{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f53705e",
   "metadata": {},
   "source": [
    "# Pixel Art Generation\n",
    "\n",
    "This Notebook shows how to generate pixel art image based on the user's prompt. Provided implementation allows for hardware acceleration for MacBook using Hugging Face *diffusers* library and pipeline interface.\n",
    "\n",
    "Used model: **nerijs/pixel-art-xl** based on **stabilityai/stable-diffusion-xl-base-1.0**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to install all needed libraries. It assumes that there are no python libraries installed in the environment.\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8441d7",
   "metadata": {},
   "source": [
    "Let's start with importing necessary libraries. We are using `DiffusionPipeline()` from `diffusers` library and `torch` for available hardware check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80196646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a606396",
   "metadata": {},
   "source": [
    "Load the base DiffusionPipeline and specify torch_dtype=torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe2c39",
   "metadata": {},
   "source": [
    "Define device based on MPS and CUDA availability. When MPS or CUDA is available, use it, otherwise use cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c58ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4998f",
   "metadata": {},
   "source": [
    "Move the pipeline to the chosen device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe716fb",
   "metadata": {},
   "source": [
    "Load the LoRA weights. They allow base model to behave in a specific way, in this example, to generate pixel art.\n",
    "\n",
    "Diffusers will automatically load these weights to the chosen device where the pipe resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_lora_weights(\"nerijs/pixel-art-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd1758",
   "metadata": {},
   "source": [
    "Specify prompt. This model doesn't need any specific phrases to generate pixel art, but you can add them. You can also specify negative prompt for better image generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"pixel art, cyberpunk hacker women working intensely on a computer in a dimly lit room at night, neons, simple, flat colors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb148a9e",
   "metadata": {},
   "source": [
    "Generate the image using created pipeline. You can control image generation by providing number of steps using num_inference_steps or the LoRA strength using cross_attention_kwargs. Here is also a place where you can ass your negative prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt,\n",
    "    # negative_prompt,\n",
    "    num_inference_steps=50, # 50 is default number of steps, adjust as desired\n",
    "    cross_attention_kwargs={\"scale\": 1} # Default scale is 1, adjust LoRA scale as needed (e.g., 0.7-1.0 is common)\n",
    ").images[0]\n",
    "\n",
    "# image = pipe(prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6a0f5",
   "metadata": {},
   "source": [
    "## Every step of image generation \n",
    "\n",
    "Using the same prepared pipeline, we can modify its callback mechanism. We will implement logic to save every step of new image generation in a specific provided files location.\n",
    "\n",
    "To do that we are following Hugging Face documentation. Generated step images will be smaller that final image to speed-up generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def latents_to_rgb(latents):\n",
    "    weights = (\n",
    "        (60, -60, 25, -70),\n",
    "        (60,  -5, 15, -50),\n",
    "        (60,  10, -5, -35)\n",
    "    )\n",
    "    \n",
    "    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))\n",
    "    biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)\n",
    "    rgb_tensor = torch.einsum(\"...lxy,lr -> ...rxy\", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)\n",
    "    image_array = rgb_tensor.clamp(0, 255)[0].byte().cpu().numpy()\n",
    "    image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "    return Image.fromarray(image_array)\n",
    "\n",
    "def decode_tensors(pipe, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "    image = latents_to_rgb(latents)\n",
    "    image.save(f\"./corgi/{step}.png\")\n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "image = pipe(\n",
    "    prompt=\"pixel art, a cute corgi in a park, flat colors.\",\n",
    "    callback_on_step_end=decode_tensors,\n",
    "    callback_on_step_end_tensor_inputs=[\"latents\"],\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09365fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36dc2b",
   "metadata": {},
   "source": [
    "Generate gif showing whole process form random noise to final image based on every step images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fe35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ffmpeg -framerate 30 -pattern_type glob -i 'corgi/*.png' -r 15 out3.gif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
